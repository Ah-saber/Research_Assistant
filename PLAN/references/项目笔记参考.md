# 项目笔记参考

本文档提供项目笔记的格式示例和最佳实践。

## 完整模板

```yaml
---
title: "基于注意力机制的时序异常检测"
type: project
tags: [项目, 异常检测, 注意力机制, 时序数据]
date: 2024-01-15
status: active
related: [["注意力机制"], ["Transformer架构"]]
---
```

## 笔记内容

## 项目概述

**目标**：研究如何使用注意力机制提高时序数据异常检测的性能和可解释性。

**动机**：
- 传统异常检测方法（如统计方法、距离方法）对复杂模式识别能力有限
- 深度学习方法（如LSTM-AE）缺乏可解释性
- 注意力机制能够突出异常发生的关键时间点

**预期贡献**：
- 提出基于注意力机制的异常检测框架
- 提供注意力可视化，增强可解释性
- 在多个数据集上验证方法有效性

## 目标

### 主要目标
1. 设计基于Transformer的时序异常检测模型
2. 实现注意力权重的可视化
3. 在公开数据集上达到SOTA性能

### 次要目标
1. 研究不同注意力机制的效果
2. 分析注意力权重与异常的关系
3. 开发可复用的代码库

## 进度

### 已完成
- [x] 文献调研（20篇相关论文）
- [x] 数据集收集和预处理
- [x] Baseline模型实现（LSTM-AE, VAE）
- [x] Transformer基础架构搭建
- [x] 初步实验（小规模验证）

### 进行中
- [ ] 改进注意力机制设计
- [ ] 大规模实验
- [ ] 可视化模块开发
- [ ] 论文撰写

### 待办
- [ ] 在3个数据集上完整测试
- [ ] 与SOTA方法对比
- [ ] 消融实验
- [ ] 用户研究（可解释性评估）

## 技术方案

### 模型架构

```
时序输入 → 位置编码 → Transformer编码器 → 重构模块 → 异常分数
                                    ↓
                              注意力权重提取
```

### 关键创新

1. **双重注意力**：
   - 时间注意力：关注异常时间点
   - 特征注意力：关注异常特征维度

2. **对比学习**：
   - 正常样本和异常样本的注意力模式对比
   - 增强异常模式的识别能力

3. **可解释性**：
   - 注意力热力图
   - 特征重要性排序

### 技术栈

- **框架**：PyTorch 2.0
- **数据处理**：Pandas, NumPy
- **可视化**：Matplotlib, Plotly
- **实验跟踪**：MLflow
- **数据集**：
  - KPI (KPI Anomaly Detection)
  - SMD (Server Machine Dataset)
  - SWaT (Secure Water Treatment)

## 实验设计

### 数据集

| 数据集 | 样本数 | 特征数 | 异常比例 |
|--------|--------|--------|----------|
| KPI | 10M | 1 | 0.5% |
| SMD | 55M | 38 | 4.2% |
| SWaT | 1M | 51 | 0.1% |

### 评估指标

- **F1-score**：精确率和召回率的调和平均
- **Precision**：检测到异常中真正异常的比例
- **Recall**：真正异常被检测到的比例
- **AUC-ROC**：ROC曲线下面积

### Baseline方法

1. **传统方法**：
   - Isolation Forest
   - One-Class SVM
   - PCA-based

2. **深度学习方法**：
   - LSTM-AE
   - VAE
   - DAGMM
   - OmniAnomaly

### 实验设置

- 训练集：仅正常数据
- 测试集：正常+异常数据
- 验证：5折交叉验证
- 随机种子：固定为42

## 初步结果

### KPI数据集（小规模测试）

| 方法 | F1 | Precision | Recall |
|------|----|----|----|
| Isolation Forest | 0.72 | 0.68 | 0.77 |
| LSTM-AE | 0.81 | 0.79 | 0.83 |
| **Transformer-AD (ours)** | **0.86** | **0.84** | **0.88** |

**初步结论**：
- Transformer-based方法优于LSTM-AE
- 注意力权重确实聚焦在异常点
- 计算成本较高，需要优化

## 问题与挑战

### 当前问题

1. **计算效率**：
   - Transformer对长序列计算成本高
   - GPU内存占用大
   - **解决方案**：使用稀疏注意力或线性attention

2. **阈值选择**：
   - 异常分数的阈值难以确定
   - 不同数据集最优阈值不同
   - **解决方案**：自适应阈值或动态阈值

3. **可解释性验证**：
   - 如何客观评估注意力可视化的有效性？
   - **解决方案**：用户研究或专家评估

### 技术挑战

1. 长序列建模（1000+时间步）
2. 多变量时序的特征交互
3. 实时检测要求（低延迟）

## 下一步计划

### 本周计划
- [ ] 实现稀疏注意力变体
- [ ] 在SMD数据集上实验
- [ ] 设计注意力可视化界面

### 本月计划
- [ ] 完成所有数据集实验
- [ ] 消融实验（分析各组件贡献）
- [ ] 开始撰写论文初稿

### 下月计划
- [ ] 与SOTA方法全面对比
- [ ] 可解释性用户研究
- [ ] 论文修改和投稿

## 资源链接

### 代码
- GitHub仓库：[链接]
- 实验配置：configs/
- 数据预处理：data/

### 文献
- [[Transformer论文]]
- [[时序异常检测综述]]
- [[注意力机制]]

### 相关项目
- [[多模态异常检测]] - 未来方向
- [[实时异常检测系统]] - 应用落地

## 会议记录

### 2024-01-20 组会讨论

**参会者**：我、导师、同组同学

**讨论要点**：
1. 当前方法计算复杂度太高
2. 建议尝试Informer或Reformer
3. 需要更严格的实验设计

**行动项**：
- [ ] 调研Informer论文
- [ ] 实现线性attention
- [ ] 补充更多baseline

### 2024-02-01 进度汇报

**汇报内容**：
- 展示初步实验结果
- 演示注意力可视化
- 讨论下一步计划

**反馈**：
- 结果有promising
- 需要更多数据集验证
- 可解释性部分需要加强

## 备注

### 数据获取
- KPI数据集已下载
- SMD需要申请（正在处理）
- SWaT已获取

### 计算资源
- GPU：NVIDIA A100 (40GB) × 2
- 预计训练时间：每个数据集2-3天
- 已申请延长使用时间

### 论文投稿计划
- 目标会议：KDD 2024
- 截稿日期：2024年2月15日
- 需要加快实验进度

## 更新历史

- 2024-01-15: 创建项目笔记
- 2024-01-20: 完成初步实验
- 2024-02-01: 组会讨论，调整方案
